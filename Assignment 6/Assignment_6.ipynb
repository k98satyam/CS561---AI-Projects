{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "def dataPreprocessing(path):\n",
    "    questions = []\n",
    "    labels = []\n",
    "    with open(path,'r',encoding='Latin_1') as data:\n",
    "        for line in data:\n",
    "            label, question = line.split(\" \", 1)\n",
    "            labels.append(label.split(\":\")[0])\n",
    "            questions.append(question)\n",
    "    return questions, labels\n",
    "\n",
    "trainingSet, trainingLabels = dataPreprocessing('train_5500')\n",
    "testingSet, testingLabels = dataPreprocessing('TREC_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training and test questions to ensure consistent TF-IDF features\n",
    "fullDataset = trainingSet + testingSet\n",
    "\n",
    "# Feature extraction: Length of the question, TF-IDF for word unigrams, and POS tag unigrams\n",
    "unigramRanges = [(1, 500), (2, 300), (3, 200)]\n",
    "\n",
    "def featureExtraction(questions, ngram_range, use_pos=False):\n",
    "    # Length of the features\n",
    "    featureLength = np.array([len(q) for q in questions])\n",
    "    \n",
    "    tfidfVectorizer = TfidfVectorizer(\n",
    "        ngram_range=ngram_range,\n",
    "        max_features=ngram_range[1],\n",
    "        stop_words='english' if ngram_range[0] == 1 else None\n",
    "    )\n",
    "    tfidf_features = tfidfVectorizer.fit_transform(questions)\n",
    "\n",
    "    if use_pos:\n",
    "        pos_tags = [nltk.pos_tag(word_tokenize(question)) for question in questions]\n",
    "        pos_tag_features = [\" \".join(tag[1] for tag in tags) for tags in pos_tags]\n",
    "        tfidf_pos_vectorizer = TfidfVectorizer(\n",
    "            ngram_range=(1, 1),\n",
    "            max_features=500\n",
    "        )\n",
    "        pos_features = tfidf_pos_vectorizer.fit_transform(pos_tag_features)\n",
    "        \n",
    "        # Combine all features\n",
    "        return np.hstack((featureLength.reshape(-1, 1), tfidf_features.toarray(), pos_features.toarray()))\n",
    "    else:\n",
    "        return np.hstack((featureLength.reshape(-1, 1), tfidf_features.toarray()))\n",
    "\n",
    "# Create feature vectors for training and test data\n",
    "all_features = featureExtraction(fullDataset, (1, 500), use_pos=True)\n",
    "train_features = all_features[:len(trainingSet)]\n",
    "test_features = all_features[len(trainingSet):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.00%\n"
     ]
    }
   ],
   "source": [
    "# Train a Decision Tree classifier\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(train_features, trainingLabels)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "test_predictions = classifier.predict(test_features)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(testingLabels, test_predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Index Criterion:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       1.00      0.78      0.88         9\n",
      "        DESC       0.75      0.94      0.84       138\n",
      "        ENTY       0.58      0.59      0.58        94\n",
      "         HUM       0.65      0.71      0.68        65\n",
      "         LOC       0.72      0.69      0.70        81\n",
      "         NUM       0.89      0.60      0.72       113\n",
      "\n",
      "    accuracy                           0.72       500\n",
      "   macro avg       0.77      0.72      0.73       500\n",
      "weighted avg       0.74      0.72      0.72       500\n",
      "\n",
      "F1-Score: 0.72\n",
      "Misclassification Error Criterion:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.88      0.78      0.82         9\n",
      "        DESC       0.73      0.91      0.81       138\n",
      "        ENTY       0.58      0.56      0.57        94\n",
      "         HUM       0.64      0.72      0.68        65\n",
      "         LOC       0.65      0.68      0.67        81\n",
      "         NUM       0.87      0.55      0.67       113\n",
      "\n",
      "    accuracy                           0.70       500\n",
      "   macro avg       0.73      0.70      0.70       500\n",
      "weighted avg       0.71      0.70      0.70       500\n",
      "\n",
      "F1-Score: 0.70\n",
      "Cross-Entropy Criterion:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.89      0.89      0.89         9\n",
      "        DESC       0.68      0.90      0.78       138\n",
      "        ENTY       0.56      0.53      0.55        94\n",
      "         HUM       0.57      0.62      0.59        65\n",
      "         LOC       0.71      0.65      0.68        81\n",
      "         NUM       0.81      0.54      0.65       113\n",
      "\n",
      "    accuracy                           0.67       500\n",
      "   macro avg       0.70      0.69      0.69       500\n",
      "weighted avg       0.68      0.67      0.67       500\n",
      "\n",
      "F1-Score: 0.67\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Create a Decision Tree classifier using the Gini index criterion\n",
    "gini_classifier = DecisionTreeClassifier(criterion='gini')\n",
    "# Train the classifier on the training data\n",
    "gini_classifier.fit(train_features, trainingLabels)\n",
    "\n",
    "# Create a Decision Tree classifier using the misclassification error criterion\n",
    "error_classifier = DecisionTreeClassifier(criterion='entropy', splitter='random')\n",
    "# Train the classifier on the training data\n",
    "error_classifier.fit(train_features, trainingLabels)\n",
    "\n",
    "# Create a Decision Tree classifier using the cross-entropy criterion\n",
    "entropy_classifier = DecisionTreeClassifier(criterion='entropy')\n",
    "# Train the classifier on the training data\n",
    "entropy_classifier.fit(train_features, trainingLabels)\n",
    "\n",
    "# # Perform 10-fold cross-validation and generate predictions\n",
    "# gini_predictions = cross_val_predict(gini_classifier, train_features, trainingLabels, cv=10)\n",
    "# error_predictions = cross_val_predict(error_classifier, train_features, trainingLabels, cv=10)\n",
    "# entropy_predictions = cross_val_predict(entropy_classifier, train_features, trainingLabels, cv=10)\n",
    "gini_predictions = gini_classifier.predict(test_features)\n",
    "error_predictions = error_classifier.predict(test_features)\n",
    "entropy_predictions = entropy_classifier.predict(test_features)\n",
    "\n",
    "# Evaluate the models using classification reports\n",
    "gini_report = classification_report(testingLabels, gini_predictions, target_names=np.unique(testingLabels))\n",
    "error_report = classification_report(testingLabels, error_predictions, target_names=np.unique(testingLabels))\n",
    "entropy_report = classification_report(testingLabels, entropy_predictions, target_names=np.unique(testingLabels))\n",
    "\n",
    "# Calculate F1-scores\n",
    "gini_f1 = f1_score(testingLabels, gini_predictions, average='weighted')\n",
    "error_f1 = f1_score(testingLabels, error_predictions, average='weighted')\n",
    "entropy_f1 = f1_score(testingLabels, entropy_predictions, average='weighted')\n",
    "\n",
    "# Print the classification reports and F1-scores\n",
    "print(\"Gini Index Criterion:\")\n",
    "print(gini_report)\n",
    "print(f\"F1-Score: {gini_f1:.2f}\")\n",
    "\n",
    "print(\"Misclassification Error Criterion:\")\n",
    "print(error_report)\n",
    "print(f\"F1-Score: {error_f1:.2f}\")\n",
    "\n",
    "print(\"Cross-Entropy Criterion:\")\n",
    "print(entropy_report)\n",
    "print(f\"F1-Score: {entropy_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Ablation Study - Feature: Length\n",
      "F1-Score (without Length): 0.60\n",
      "F1-Score Change: 0.01\n",
      "Feature Ablation Study - Feature: TF-IDF Word Unigrams\n",
      "F1-Score (without TF-IDF Word Unigrams): 0.61\n",
      "F1-Score Change: 0.00\n",
      "Feature Ablation Study - Feature: POS Tag Unigrams\n",
      "F1-Score (without POS Tag Unigrams): 0.60\n",
      "F1-Score Change: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Feature Ablation Study\n",
    "features = [\"Length\", \"TF-IDF Word Unigrams\", \"POS Tag Unigrams\"]\n",
    "original_f1 = gini_f1  # F1-score with all features\n",
    "\n",
    "for i, feature_name in enumerate(features):\n",
    "    # Create a copy of all features with the current feature removed\n",
    "    ablated_features = np.delete(train_features, i, axis=1)\n",
    "    \n",
    "    # Perform 10-fold cross-validation using the ablated features\n",
    "    ablated_predictions = cross_val_predict(gini_classifier, ablated_features, trainingLabels, cv=10)\n",
    "    \n",
    "    # Calculate F1-score for the ablated feature set\n",
    "    ablated_f1 = f1_score(trainingLabels, ablated_predictions, average='weighted')\n",
    "    \n",
    "    # Calculate the change in F1-score\n",
    "    f1_change = original_f1 - ablated_f1\n",
    "    \n",
    "    print(f\"Feature Ablation Study - Feature: {feature_name}\")\n",
    "    print(f\"F1-Score (without {feature_name}): {ablated_f1:.2f}\")\n",
    "    print(f\"F1-Score Change: {f1_change:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Propagation Analysis:\n",
      "Gini to Error Propagation: 30.63%\n",
      "Error to Gini Propagation: 31.23%\n",
      "Entropy to Error Propagation: 33.53%\n",
      "Error to Entropy Propagation: 28.98%\n"
     ]
    }
   ],
   "source": [
    "# Error Propagation Analysis\n",
    "def error_propagation_analysis(original_predictions, corrected_predictions):\n",
    "    # Calculate the number of samples misclassified by the original model but correctly classified by the corrected model\n",
    "    misclassified_original_corrected = sum((original_predictions != trainingLabels) & (corrected_predictions == trainingLabels))\n",
    "    total_misclassified_original = sum(original_predictions != trainingLabels)\n",
    "    if total_misclassified_original == 0:\n",
    "        return 0.0  # Avoid division by zero\n",
    "    return (misclassified_original_corrected / total_misclassified_original) * 100\n",
    "\n",
    "# Analyze error propagation between models\n",
    "gini_error_propagation = error_propagation_analysis(gini_predictions, error_predictions)\n",
    "error_gini_propagation = error_propagation_analysis(error_predictions, gini_predictions)\n",
    "entropy_error_propagation = error_propagation_analysis(entropy_predictions, error_predictions)\n",
    "error_entropy_propagation = error_propagation_analysis(error_predictions, entropy_predictions)\n",
    "\n",
    "print(\"Error Propagation Analysis:\")\n",
    "print(f\"Gini to Error Propagation: {gini_error_propagation:.2f}%\")\n",
    "print(f\"Error to Gini Propagation: {error_gini_propagation:.2f}%\")\n",
    "print(f\"Entropy to Error Propagation: {entropy_error_propagation:.2f}%\")\n",
    "print(f\"Error to Entropy Propagation: {error_entropy_propagation:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.61\n",
      "Average Recall: 0.60\n",
      "Average F1-score: 0.60\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "num_folds = 10\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier (or any other classifier you prefer)\n",
    "classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Initialize lists to store evaluation metrics\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Create StratifiedKFold object to ensure class balance in each fold\n",
    "stratified_kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for train_index, val_index in stratified_kfold.split(train_features, trainingLabels):\n",
    "    # Split the data into training and validation sets for this fold\n",
    "    X_train_fold, X_val_fold = train_features[train_index], train_features[val_index]\n",
    "    y_train_fold, y_val_fold = np.array(trainingLabels)[train_index], np.array(trainingLabels)[val_index]\n",
    "\n",
    "    # Train the classifier on the training data for this fold\n",
    "    classifier.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Make predictions on the validation data for this fold\n",
    "    y_pred = classifier.predict(X_val_fold)\n",
    "\n",
    "    # Calculate precision, recall, and F-score for this fold\n",
    "    precision = precision_score(y_val_fold, y_pred, average='weighted')\n",
    "    recall = recall_score(y_val_fold, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_val_fold, y_pred, average='weighted')\n",
    "\n",
    "    # Append the scores to the respective lists\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate the average scores over all folds\n",
    "avg_precision = np.mean(precision_scores)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "# Print the average scores\n",
    "print(\"Average Precision: {:.2f}\".format(avg_precision))\n",
    "print(\"Average Recall: {:.2f}\" .format(avg_recall))\n",
    "print(\"Average F1-score: {:.2f}\" .format(avg_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.61 Recall: 0.61 F1_score: 0.61\n",
      "Precision: 0.63 Recall: 0.62 F1_score: 0.62\n",
      "Precision: 0.59 Recall: 0.59 F1_score: 0.59\n",
      "Precision: 0.58 Recall: 0.58 F1_score: 0.58\n",
      "Precision: 0.61 Recall: 0.60 F1_score: 0.60\n",
      "Precision: 0.61 Recall: 0.60 F1_score: 0.60\n",
      "Precision: 0.57 Recall: 0.57 F1_score: 0.57\n",
      "Precision: 0.62 Recall: 0.61 F1_score: 0.62\n",
      "Precision: 0.62 Recall: 0.62 F1_score: 0.62\n",
      "Precision: 0.64 Recall: 0.63 F1_score: 0.63\n"
     ]
    }
   ],
   "source": [
    "for p, r, f1 in zip(precision_scores, recall_scores, f1_scores):\n",
    "    print(f\"Precision: {p:.2f} Recall: {r:.2f} F1_score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
